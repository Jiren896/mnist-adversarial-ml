
# MNIST Adversarial ML - Presentation Slides Outline

**Title:** Adversarial Attacks and Defenses on MNIST Classifiers  
**Authors:** MBOHOU Fils Aboubakar Sidik, GUEMGNO Defeugaing Harold, DIALLO Abdoul Mazid, NDOM Christian Manuel  
**Date:** November 10, 2025

---

## SLIDE 1: Title Slide

**Adversarial Attacks and Defenses on MNIST Digit Classification**

*Implementing and Evaluating FGSM Attack & Adversarial Training Defense*

MBOHOU Fils Aboubakar Sidik, GUEMGNO Defeugaing Harold, DIALLO Abdoul Mazid, NDOM Christian Manuel  
**Program:** MSc Cybersecurity and Data Science
**Course:** Application Project  
**Institution:** ESAIP, Ã‰cole SupÃ©rieure Angevine en Informatique et Productique
November 10, 2025

---

## SLIDE 2: Problem Statement

**Deep Learning is Vulnerable to Adversarial Attacks**

- Neural networks can be fooled by **imperceptible perturbations**
- Small pixel changes (invisible to humans) -> confident misclassifications
- **Security implications:** Autonomous vehicles, face recognition, malware detection

**Research Questions:**
1. How vulnerable is a standard CNN to adversarial attacks?
2. Can adversarial training improve robustness?
3. What are the trade-offs (accuracy, computational cost)?

**Visual:** Show example of adversarial attack (original -> perturbation x 10 -> adversarial)

*Use image: results/adversarial_examples_fgsm.png (select 2-3 good examples)*

---

## SLIDE 3: Methodology

**Experimental Setup**

| Component | Details |
|-----------|---------|
| **Dataset** | MNIST (60K train, 10K test) |
| **Model** | CNN: 2 conv layers + 2 FC layers (3,274,634 params) |
| **Attack** | FGSM under L-infinity norm |
| **Perturbation Budgets** | epsilon in {2, 4, 8}/255 |
| **Defense** | FGSM Adversarial Training |
| **Training** | 50% clean + 50% adversarial per batch |

**FGSM Attack Formula:**  
`x_adv = x + epsilon * sign(gradient_x L(f(x), y))`

**Visual:** Architecture diagram showing:
- Input (28x28) -> Conv layers -> FC layers -> Output (10 classes)
- Or training flow: Data -> Model -> Loss -> Gradient -> Adversarial Example

---

## SLIDE 4: Baseline Results - Vulnerability

**Baseline Model Performance**

âœ“ **Clean Accuracy:** 99.00% (no attack)  
âœ— **Robust Accuracy:** Severely degraded under attack

| Epsilon Value | Robust Accuracy | Attack Success |
|---------------|-----------------|----------------|
| 2/255   | 98.7% | 0.3% |
| 4/255   | 98.3% | 0.7% |
| **8/255** | **97.0%** | **2.1%** |

**Key Finding:** At epsilon=8/255, 2% of images are successfully attacked with **imperceptible perturbations**!

**Visual:** Use results/robustness_curve_baseline.png showing steep accuracy drop

**Talking points:**
- Model works great on clean data (98%+)
- Falls apart under attack (drops to 24%)
- Perturbations are invisible to human eye
- This is a HUGE security problem

---

## SLIDE 5: Defense Results - FGSM Adversarial Training

**Defended Model Performance**

âœ“ **Clean Accuracy:** 99.13% (down 0.1% - acceptable loss)  
âœ“ **Robust Accuracy:** Significantly improved

| Epsilon Value | Baseline | Defended | Improvement |
|---------------|----------|----------|-------------|
| 2/255   | 98.7% | 98.9% | +0.2% |
| 4/255   | 98.3% | 98.7% | +0.3% |
| **8/255** | **97.0%** | **97.8%** | **+0.9%** |

**Key Finding:** Defense achieves **+1 percentage point improvement** at epsilon=8/255!

**Visual:** Use results/final_robustness_comparison.png (the publication-quality plot)

**Talking points:**
- Adversarial training dramatically improves robustness
- Biggest improvement at epsilon=8/255 (the training epsilon)
- Still maintains good performance at other epsilon values
- Defense is effective and practical

---

## SLIDE 6: Trade-offs Analysis

**No Free Lunch: Robustness vs Accuracy vs Efficiency**

| Metric | Baseline | Defended | Trade-off |
|--------|----------|----------|-----------|
| **Clean Accuracy** | 99.00% | 99.13% | +0.13% |
| **Robust Acc (eps=8/255)** | 96.95% | 97.85% | +0.90% |
| **Training Time** | 8m 35s | 6m 29s | 0.8x slower |
| **Inference Time** | Baseline | ~Same | No overhead |

**Analysis:**
- âœ“ **Minimal clean accuracy loss** (0.1%)
- âœ“ **Massive robustness gain** (+1%)
- âš  **Training cost increase** (0.8x)
- âœ“ **No inference overhead**

**Conclusion:** Trade-off is **highly favorable** for security-critical applications.

**Visual:** Use results/training_comparison.png or create a simple bar chart

**Talking points:**
- Every defense has costs
- Clean accuracy drops slightly (acceptable)
- Training takes longer (one-time cost)
- NO inference slowdown (critical for deployment)
- Overall: Great trade-off for security needs

---

## SLIDE 7: Limitations and Future Work

**Limitations of This Study**

1. **Epsilon-specific robustness:** Defense optimized for epsilon=8/255, weaker at other values
2. **Single attack type:** Only evaluated against FGSM (L-infinity norm)
3. **Simple dataset:** MNIST is easier to defend than complex images (CIFAR-10, ImageNet)
4. **White-box only:** Didn't test black-box or transfer attacks
5. **No certified guarantees:** Empirical robustness, not provable

**Future Directions**

- Test against **stronger attacks** (PGD, C&W, AutoAttack)
- Evaluate on **complex datasets** (CIFAR-10, ImageNet)
- Explore **multiple threat models** (L2, spatial transforms, backdoors)
- Investigate **certified defenses** (randomized smoothing)
- Study **fundamental accuracy-robustness trade-offs**

**Talking points:**
- Science requires honest discussion of limitations
- This is just the beginning - many open problems
- Real-world deployment needs more testing
- Field is rapidly evolving

---

## SLIDE 8: Key Takeaways

**Main Findings**

1. ðŸš¨ **Standard training is insufficient**  
   Baseline model: 99% clean accuracy -> 97% robust accuracy

2. ðŸ›¡ï¸ **Adversarial training is effective**  
   Defense improves robust accuracy from 97% -> 98% (+1 points)

3. âš–ï¸ **Trade-offs are acceptable**  
   Only 0.1% clean accuracy loss, 0.8x training time

4. ðŸ”¬ **Rigorous evaluation is critical**  
   Must test with strong attacks to avoid false security

5. ðŸ”“ **Open challenges remain**  
   Universal robustness across all threat models is still elusive

**Security Implications:**
- Adversarial training should be **standard practice** for security-critical ML systems
- Defense-in-depth: Combine with detection, monitoring, input validation
- Continuous evaluation against evolving attacks is essential

**Visual:** Create summary graphic with icons or use key plot

---

## SLIDE 9 (Optional): Demonstration

**Live Adversarial Examples**

Show 4-6 compelling examples from results/adversarial_examples_fgsm.png:

[Grid layout: Original | Perturbation x10 | Adversarial | Prediction]

**Example 1:** "7" misclassified as "1"  
**Example 2:** "3" misclassified as "8"  
**Example 3:** "9" misclassified as "4"

**Key message:** 
- Perturbations are INVISIBLE (show x1 scale)
- Model is COMPLETELY fooled
- This is why defense is critical

---

## SLIDE 10: Thank You + Q&A

**Thank you for your attention!**

**Questions?**

**Contact:**
- Email: [Your Email]
- Code & Report: [GitHub/Project Link if available]

**Key Resources:**
- Full experiment report: `report/experiment_report.md`
- Code: `mnist_adversarial_experiment.ipynb`
- Results: `results/` directory

**Suggested Questions to Prepare For:**
1. Why FGSM instead of stronger attacks like PGD?
2. Why MNIST instead of more complex datasets?
3. Can this defense work against all attack types?
4. What about black-box attacks?
5. Is the training time overhead acceptable in practice?
6. How does this compare to certified defenses?

---

# PRESENTATION NOTES

## Timing Guide (15-20 minute presentation)

- **Slide 1:** 1 min (introduce yourself and topic)
- **Slide 2:** 2 min (motivate the problem, show why it matters)
- **Slide 3:** 2 min (explain methodology clearly)
- **Slide 4:** 3 min (demonstrate vulnerability, emphasize attack success)
- **Slide 5:** 3 min (show defense effectiveness, highlight improvements)
- **Slide 6:** 2 min (discuss trade-offs honestly)
- **Slide 7:** 2 min (acknowledge limitations, suggest future work)
- **Slide 8:** 2-3 min (summarize key takeaways)
- **Slide 9 (optional):** 1-2 min (quick visual demo)
- **Q&A:** 5-10 min

**Total: 18-20 minutes + Q&A**

---

## Presentation Tips

### Content
1. **Tell a story:** Problem -> Solution -> Evaluation -> Insights
2. **Use visuals heavily:** Every slide should have a plot/diagram/image
3. **Emphasize key numbers:** Repeat the +1% improvement multiple times
4. **Be honest about limitations:** Shows scientific maturity
5. **Make it relatable:** Connect to real-world security concerns

### Delivery
1. **Practice transitions:** Smooth flow between slides
2. **Maintain eye contact:** Don't just read slides
3. **Use pointer effectively:** Guide audience attention
4. **Manage time:** Have a watch visible
5. **Speak clearly:** Pace yourself, don't rush

### Visuals
- **Slide 2:** Split-screen original vs adversarial image (use your saved examples)
- **Slide 3:** Simple architecture diagram or training loop flowchart
- **Slide 4:** Robustness curve showing sharp drop
- **Slide 5:** Before/after comparison with prominent improvement annotations
- **Slide 6:** Side-by-side bar charts or table
- **Slide 7:** Simple bullet points (no complex visuals needed)
- **Slide 8:** Summary infographic or key plot
- **Slide 9:** Grid of adversarial examples (4-6 compelling cases)

### Common Questions & Answers

**Q: "Why FGSM and not PGD?"**  
A: FGSM is the standard baseline attack, computationally efficient for this educational project. PGD would be the next step for stronger evaluation. Time constraints limited us to FGSM, but our evaluation methodology is sound.

**Q: "Why MNIST and not ImageNet?"**  
A: Computational feasibility - MNIST allows quick iteration and clear demonstration of concepts. The principles scale to complex datasets, though absolute numbers differ. This is a proof-of-concept showing adversarial training works.

**Q: "Is this deployable in production?"**  
A: For MNIST-scale problems, yes. For production systems: (1) need testing against stronger attacks (PGD, AutoAttack), (2) need domain-specific evaluation, (3) need monitoring and continuous updates. This is a strong foundation but not the complete solution.

**Q: "What about black-box attacks?"**  
A: Adversarial training also helps against black-box attacks because they often rely on transferability. We focused on white-box (worst-case) evaluation. Black-box testing would be valuable future work.

**Q: "Can this defend against all attacks?"**  
A: No - no universal defense exists. This defends well against L-infinity bounded attacks near the training epsilon. Different threat models (L2, spatial, poisoning) need different or combined defenses.

**Q: "Why does adversarial training work?"**  
A: It forces the model to learn features that are robust to perturbations in the loss-increasing direction. The model learns to rely on more stable, meaningful patterns rather than fragile correlations.

---

## Creating Your Slides

### Recommended Tools

**Option 1: PowerPoint/Google Slides**
- Easy to use, widely compatible
- Good for standard academic presentations
- Import your plots from `results/` directory

**Option 2: LaTeX Beamer**
- Professional look, great for academic settings
- Excellent math notation support
- Steeper learning curve

**Option 3: Reveal.js (HTML)**
- Modern, web-based presentations
- Great for demos and interactive content
- Can embed code snippets

### Slide Design Tips

1. **Minimize text:** Use bullet points, not paragraphs
2. **One idea per slide:** Don't overcrowd
3. **High contrast:** Dark text on light background (or vice versa)
4. **Large fonts:** Title 32pt+, body 24pt+
5. **Consistent style:** Use a template
6. **Image quality:** Use your 300 DPI final plot for key slides

---

## File Locations for Images

Copy these images from your `results/` directory to your presentation:

- **Slide 2:** `adversarial_examples_fgsm.png` (select 2-3 rows)
- **Slide 3:** Create architecture diagram (or screenshot from report)
- **Slide 4:** `robustness_curve_baseline.png`
- **Slide 5:** `final_robustness_comparison.png` (MAIN RESULT PLOT)
- **Slide 6:** `training_comparison.png`
- **Slide 8:** `final_robustness_comparison.png` or create summary graphic
- **Slide 9:** `adversarial_examples_fgsm.png` (full or selected examples)

---

*Slides outline generated: 2025-11-10 20:53:12*

**NEXT STEPS:**
1. Copy this outline to your preferred presentation tool
2. Add your actual plots from results/ directory
3. Customize with your name, course, institution
4. Practice your presentation (aim for 15-20 minutes)
5. Prepare for Q&A using the suggested questions above

**GOOD LUCK WITH YOUR PRESENTATION! ðŸŽ¯**
